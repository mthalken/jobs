{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e3de01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Data Scientist Atlanta, GA\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24740/3568077606.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    163\u001b[0m                     \u001b[1;34m\"Denver, CO\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Dallas-Ft. Worth, TX\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Los Angeles, CA\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \"New York City, NY\", \"San Francisco, CA\", \"Seattle, WA\"]\n\u001b[1;32m--> 165\u001b[1;33m     \u001b[0mjob_loc_scrape_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprimary_city_state_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'7'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     secondary_city_state_list = [\"Phoenix, AZ\", \"Salt Lake City, UT\", \"San Antonio, TX\", \n\u001b[0;32m    167\u001b[0m                     \u001b[1;34m\"San Diego, CA\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Jacksonville, FL\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Columbus, OH\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Boise, ID\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24740/3568077606.py\u001b[0m in \u001b[0;36mjob_loc_scrape_loop\u001b[1;34m(job_list, loc_list, job_age)\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mindeed_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_indeed_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_age\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[1;31m# indeed_response = web_scrape_api_call(indeed_url)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0mresult_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_job_page_meta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindeed_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m             \u001b[0mresult_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'retrieve_date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dumping to sql'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24740/3568077606.py\u001b[0m in \u001b[0;36mscrape_job_page_meta\u001b[1;34m(job_page_html)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mjobs_df\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mscraped\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mjob\u001b[0m \u001b[0msearch\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     ''' \n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mpage_soup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_page_html\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[0mdf_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'job_title'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'company_name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'company_location'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'est_salary'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'job_href'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'job_desc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0mjobs_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "# from config import WebScrapingAPIkey\n",
    "# import sqlite3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "import selenium\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "def make_indeed_url(search_job, search_location, job_age):\n",
    "    '''\n",
    "    This function takes in 3 search parameters and inserts them into an\n",
    "    indeed.com url to search for jobs in those parameters\n",
    "    input:\n",
    "        search_job (str): job title being searched for\n",
    "        search_location (str): city, state being searched\n",
    "        job_age (int): 3 or 7, max age of job posting in days\n",
    "    output:\n",
    "        indeed_job_url (str): url to indeed jobs of the given parameters\n",
    "    '''\n",
    "    job = search_job.replace(' ', '%20')\n",
    "    location = search_location.replace(',', '%2C').replace(' ', '%20')\n",
    "    getVars = {'q': job, 'l': location, 'fromage': job_age}\n",
    "    # indeed_job_url = f'https://www.indeed.com/jobs?q={job}&l={location}&fromage={job_age}'\n",
    "    url = ('https://www.indeed.com/jobs?' + urllib.parse.urlencode(getVars)) \n",
    "    return url\n",
    "\n",
    "def scrape_job_card(job_meta):\n",
    "    '''\n",
    "    This function takes in a job_card_element from indeed.com and extracts the\n",
    "    job title, company name, company location, and estimated salary\n",
    "    input: \n",
    "        job_card_element, selenium webdriver object (specific to indeed.com)\n",
    "    output: \n",
    "        - job_title, str\n",
    "        - company_name, str\n",
    "        - company_location, str\n",
    "        - estimated_salary, str\n",
    "    '''\n",
    "    try:\n",
    "        job_title = job_meta.find('h2', {'class':'jobTitle'}).get_text().lstrip('new\\n')\n",
    "    except:\n",
    "        job_title = 'No job title found'\n",
    "    try:\n",
    "        company_name = job_meta.find('span',{'class':'companyName'}).get_text()\n",
    "    except:\n",
    "        company_name = 'No Company Name'\n",
    "    try:\n",
    "        company_location = job_meta.find('div', {'class':'companyLocation'}).get_text()\n",
    "    except:\n",
    "        company_location = 'No Location'\n",
    "    try:\n",
    "        estimated_salary = job_meta.find('div', {'class':'metadata salary-snippet-container'}).get_text()\n",
    "    except:\n",
    "        estimated_salary = 'No Estimated Salary'\n",
    "    return job_title, company_name, company_location, estimated_salary\n",
    "\n",
    "# def scrape_job_description(job_desc_href):\n",
    "#     '''\n",
    "#     This function takes in a job_card_element from indeed.com and extracts the\n",
    "#     job description.\n",
    "#     input: \n",
    "#         job_card_element: selenium webdriver object (specific to indeed.com)\n",
    "#     output: \n",
    "#         job_desc, str, can be extremely long (avg 3,000-7,000 characters)\n",
    "#     '''\n",
    "#     try:\n",
    "#         # page = web_scrape_api_call(job_desc_href)\n",
    "#         url = ('https://www.indeed.com/jobs?')\n",
    "#         params = {}\n",
    "#         # response = requests.request(\"GET\", url, params=params)\n",
    "#         soup = bs(page.content, 'html.parser')\n",
    "#         job_desc = soup.find(id='jobDescriptionText')\n",
    "#         job_desc = job_desc.text.replace('\\n', ' ').replace('\\r', '')\n",
    "#     except:\n",
    "#         job_desc = 'No Job Description'\n",
    "#     return job_desc\n",
    "\n",
    "def scrape_job_page_meta(job_page_html):\n",
    "    '''\n",
    "    This function takes in a html job page and uses beautiful soup to extract each jobs title, company name,\n",
    "    estimated salary, job description href and then uses that href to open the job description page and\n",
    "    extract that job description. While its looping through each job on the job page it is storing the \n",
    "    information in a pandas dataframe.\n",
    "    input:\n",
    "        job_page_html: html response from indeed search request\n",
    "    output:\n",
    "        jobs_df: pandas dataframe containing the scraped data from the job search page\n",
    "    ''' \n",
    "    page_soup = bs(job_page_html, 'lxml')\n",
    "    df_columns = ['job_title', 'company_name', 'company_location', 'est_salary', 'job_href','job_desc']\n",
    "    jobs_df = pd.DataFrame(columns = df_columns)\n",
    "    for job in page_soup.find_all('div',{\"id\":\"mosaic-provider-jobcards\"}): \n",
    "        # Lets find the job title\n",
    "        for href_post in job.find_all('a', href=True):\n",
    "            if href_post.find('a', href=True):\n",
    "                #this is for the url for the job posting\n",
    "                job_desc_href = 'https://www.indeed.com'+str(href_post['href'])\n",
    "                # job_desc = scrape_job_description(job_desc_href)\n",
    "                for job_meta in href_post.find_all('div',{\"class\":\"job_seen_beacon\"}):\n",
    "                    job_title, company_name, company_location, estimated_salary = scrape_job_card(job_meta)\n",
    "                    print(f'{job_title}, {job_desc_href}')        \n",
    "                    job_dict = {'job_title': job_title,\n",
    "                                'company_name': [company_name],\n",
    "                                'company_location': [company_location],\n",
    "                                'est_salary': [estimated_salary],\n",
    "                                'job_href': [job_desc_href]\n",
    "                                # 'job_desc': [job_desc]\n",
    "                                }\n",
    "                    j_df = pd.DataFrame.from_dict(job_dict)\n",
    "                    jobs_df= jobs_df.append(j_df, ignore_index=True)\n",
    "    return jobs_df\n",
    "\n",
    "\n",
    "\n",
    "def job_loc_scrape_loop(job_list,loc_list,job_age):\n",
    "    '''\n",
    "    This function takes in a list of job titles, locations and age.\n",
    "    It then loops through each item of both lists, creates a url to call\n",
    "    and then scrapes the given info from each page.\n",
    "    At the end of each item in the loop it saves the job info to a sql table\n",
    "    '''\n",
    "    date = dt.datetime.today().strftime('%Y-%m-%d')\n",
    "    for job in job_list:\n",
    "        for loc in loc_list:\n",
    "            print(f'Scraping: {job} {loc}')\n",
    "            indeed_url = make_indeed_url(job, loc, job_age)\n",
    "            # indeed_response = web_scrape_api_call(indeed_url)\n",
    "            result_df = scrape_job_page_meta(indeed_url)\n",
    "            result_df['retrieve_date'] = date\n",
    "            # print('dumping to sql')\n",
    "            # sql_dump(result_df, 'data/jobs', 'indeed_jobs')\n",
    "            return result_df, 'data/jobs', 'indeed_jobs'\n",
    "        return result_df, 'data/jobs', 'indeed_jobs'\n",
    "    return result_df, 'data/jobs', 'indeed_jobs'\n",
    "\n",
    "# def sql_dump(df, db, table):\n",
    "#     con = sqlite3.connect(db) #db=\"data\\jobs.db\"\n",
    "#     df.to_sql(table, con, if_exists='append') #table='jobs_data'\n",
    "#     con.close()\n",
    "\n",
    "# def web_scrape_api_call(url_to_scrape):\n",
    "    # '''\n",
    "    # sends the url that we would like to scrape to the webscrapingapi\n",
    "    # so that our calls can be ananomyzed. \n",
    "    # '''\n",
    "    # url = \"https://api.webscrapingapi.com/v1\"\n",
    "    # params = {\n",
    "    # \"api_key\":WebScrapingAPIkey,\n",
    "    # \"url\":url_to_scrape\n",
    "    # }\n",
    "    # response = requests.request(\"GET\", url, params=params)\n",
    "    # return response\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    job_list = [\"Data Scientist\", \"Data Analyst\", \"Data Engineer\", \n",
    "                \"Machine Learning Engineer\", \"Business Intelligence Analyst\"]\n",
    "    primary_city_state_list = [\"Atlanta, GA\", \"Austin, TX\", \"Boston, MA\", \"Chicago, IL\", \n",
    "                    \"Denver, CO\", \"Dallas-Ft. Worth, TX\", \"Los Angeles, CA\",\n",
    "                    \"New York City, NY\", \"San Francisco, CA\", \"Seattle, WA\"]\n",
    "    job_loc_scrape_loop(job_list, primary_city_state_list, '7')\n",
    "    secondary_city_state_list = [\"Phoenix, AZ\", \"Salt Lake City, UT\", \"San Antonio, TX\", \n",
    "                    \"San Diego, CA\", \"Jacksonville, FL\", \"Columbus, OH\", \"Boise, ID\",\n",
    "                    \"Washington, DC\", \"Portland, OR\", \"Kansas City\", \"Raleigh, NC\", \n",
    "                    \"Boulder, CO\", \"Miami, FL\", \"Northern Virginia\", \"Orlando, FL\"]\n",
    "\n",
    "    \n",
    "\n",
    "    job_loc_scrape_loop(job_list, secondary_city_state_list, '7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f41eb24",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extracted_info_indeed() missing 1 required positional argument: 'desired_char'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12964/3698362359.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# desired_char = ['titles', 'companies', 'links', 'date_listed']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfind_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Indeed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data analyst'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'remote'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_soup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12964/3116980842.py\u001b[0m in \u001b[0;36mfind_jobs\u001b[1;34m(website, job_title, location, filename)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwebsite\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Indeed'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mjob_soup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_indeed_jobs_div\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_title\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mjobs_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextracted_info_indeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_soup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# add other job boards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: extracted_info_indeed() missing 1 required positional argument: 'desired_char'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# desired_char = ['titles', 'companies', 'links', 'date_listed']\n",
    "\n",
    "find_jobs('Indeed', 'data analyst', 'remote')\n",
    "print(job_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64004526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
